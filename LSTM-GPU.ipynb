{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "#import mxnet as mx\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"pride_and_prejudice.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text + open(\"hamlet.txt\").read()\n",
    "#raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  911214\n",
      "Total Vocab:  86\n",
      "Characters to int:\n",
      " {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, ',': 12, '-': 13, '.': 14, '/': 15, '0': 16, '1': 17, '2': 18, '3': 19, '4': 20, '5': 21, '6': 22, '7': 23, '8': 24, '9': 25, ':': 26, ';': 27, '?': 28, '@': 29, 'A': 30, 'B': 31, 'C': 32, 'D': 33, 'E': 34, 'F': 35, 'G': 36, 'H': 37, 'I': 38, 'J': 39, 'K': 40, 'L': 41, 'M': 42, 'N': 43, 'O': 44, 'P': 45, 'Q': 46, 'R': 47, 'S': 48, 'T': 49, 'U': 50, 'V': 51, 'W': 52, 'X': 53, 'Y': 54, 'Z': 55, '[': 56, ']': 57, '_': 58, 'a': 59, 'b': 60, 'c': 61, 'd': 62, 'e': 63, 'f': 64, 'g': 65, 'h': 66, 'i': 67, 'j': 68, 'k': 69, 'l': 70, 'm': 71, 'n': 72, 'o': 73, 'p': 74, 'q': 75, 'r': 76, 's': 77, 't': 78, 'u': 79, 'v': 80, 'w': 81, 'x': 82, 'y': 83, 'z': 84, '|': 85}\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "print(\"Characters to int:\\n\",char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  911213\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "#seq_length = 30\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNLSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-capital-seq1-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "csv_logger = CSVLogger('training.log')\n",
    "tensorboard = TensorBoard(log_dir='./logs')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint,csv_logger,tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "911213/911213 [==============================] - 198s 218us/step - loss: 2.9588\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.95882, saving model to weights-improvement-capital-seq1-01-2.9588.hdf5\n",
      "Epoch 2/20\n",
      "911213/911213 [==============================] - 194s 213us/step - loss: 2.8418\n",
      "\n",
      "Epoch 00002: loss improved from 2.95882 to 2.84177, saving model to weights-improvement-capital-seq1-02-2.8418.hdf5\n",
      "Epoch 3/20\n",
      "911213/911213 [==============================] - 196s 215us/step - loss: 2.7806\n",
      "\n",
      "Epoch 00003: loss improved from 2.84177 to 2.78059, saving model to weights-improvement-capital-seq1-03-2.7806.hdf5\n",
      "Epoch 4/20\n",
      "911213/911213 [==============================] - 190s 209us/step - loss: 2.7429\n",
      "\n",
      "Epoch 00004: loss improved from 2.78059 to 2.74290, saving model to weights-improvement-capital-seq1-04-2.7429.hdf5\n",
      "Epoch 5/20\n",
      "911213/911213 [==============================] - 190s 209us/step - loss: 2.7180\n",
      "\n",
      "Epoch 00005: loss improved from 2.74290 to 2.71796, saving model to weights-improvement-capital-seq1-05-2.7180.hdf5\n",
      "Epoch 6/20\n",
      "911213/911213 [==============================] - 191s 210us/step - loss: 2.6983\n",
      "\n",
      "Epoch 00006: loss improved from 2.71796 to 2.69834, saving model to weights-improvement-capital-seq1-06-2.6983.hdf5\n",
      "Epoch 7/20\n",
      "911213/911213 [==============================] - 191s 210us/step - loss: 2.6705\n",
      "\n",
      "Epoch 00007: loss improved from 2.69834 to 2.67049, saving model to weights-improvement-capital-seq1-07-2.6705.hdf5\n",
      "Epoch 8/20\n",
      "911213/911213 [==============================] - 191s 210us/step - loss: 2.6523\n",
      "\n",
      "Epoch 00008: loss improved from 2.67049 to 2.65235, saving model to weights-improvement-capital-seq1-08-2.6523.hdf5\n",
      "Epoch 9/20\n",
      "911213/911213 [==============================] - 191s 210us/step - loss: 2.6386\n",
      "\n",
      "Epoch 00009: loss improved from 2.65235 to 2.63855, saving model to weights-improvement-capital-seq1-09-2.6386.hdf5\n",
      "Epoch 10/20\n",
      "911213/911213 [==============================] - 193s 212us/step - loss: 2.6269\n",
      "\n",
      "Epoch 00010: loss improved from 2.63855 to 2.62687, saving model to weights-improvement-capital-seq1-10-2.6269.hdf5\n",
      "Epoch 11/20\n",
      "911213/911213 [==============================] - 200s 219us/step - loss: 2.6183\n",
      "\n",
      "Epoch 00011: loss improved from 2.62687 to 2.61834, saving model to weights-improvement-capital-seq1-11-2.6183.hdf5\n",
      "Epoch 12/20\n",
      "911213/911213 [==============================] - 200s 219us/step - loss: 2.6099\n",
      "\n",
      "Epoch 00012: loss improved from 2.61834 to 2.60989, saving model to weights-improvement-capital-seq1-12-2.6099.hdf5\n",
      "Epoch 13/20\n",
      "911213/911213 [==============================] - 201s 221us/step - loss: 2.6039\n",
      "\n",
      "Epoch 00013: loss improved from 2.60989 to 2.60390, saving model to weights-improvement-capital-seq1-13-2.6039.hdf5\n",
      "Epoch 14/20\n",
      "911213/911213 [==============================] - 204s 223us/step - loss: 2.5972\n",
      "\n",
      "Epoch 00014: loss improved from 2.60390 to 2.59723, saving model to weights-improvement-capital-seq1-14-2.5972.hdf5\n",
      "Epoch 15/20\n",
      "911213/911213 [==============================] - 204s 224us/step - loss: 2.5919\n",
      "\n",
      "Epoch 00015: loss improved from 2.59723 to 2.59189, saving model to weights-improvement-capital-seq1-15-2.5919.hdf5\n",
      "Epoch 16/20\n",
      "911213/911213 [==============================] - 206s 226us/step - loss: 2.5869\n",
      "\n",
      "Epoch 00016: loss improved from 2.59189 to 2.58690, saving model to weights-improvement-capital-seq1-16-2.5869.hdf5\n",
      "Epoch 17/20\n",
      "911213/911213 [==============================] - 207s 227us/step - loss: 2.5840\n",
      "\n",
      "Epoch 00017: loss improved from 2.58690 to 2.58401, saving model to weights-improvement-capital-seq1-17-2.5840.hdf5\n",
      "Epoch 18/20\n",
      "911213/911213 [==============================] - 210s 230us/step - loss: 2.5798\n",
      "\n",
      "Epoch 00018: loss improved from 2.58401 to 2.57985, saving model to weights-improvement-capital-seq1-18-2.5798.hdf5\n",
      "Epoch 19/20\n",
      "911213/911213 [==============================] - 211s 231us/step - loss: 2.5762\n",
      "\n",
      "Epoch 00019: loss improved from 2.57985 to 2.57615, saving model to weights-improvement-capital-seq1-19-2.5762.hdf5\n",
      "Epoch 20/20\n",
      "911213/911213 [==============================] - 212s 232us/step - loss: 2.5730\n",
      "\n",
      "Epoch 00020: loss improved from 2.57615 to 2.57300, saving model to weights-improvement-capital-seq1-20-2.5730.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x161cd973ac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
